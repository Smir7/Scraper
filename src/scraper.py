import schedule
import time
import requests
from bs4 import BeautifulSoup
import os


def get_book_data(url):
    """
    –ü–æ–ª—É—á–∞–µ—Ç –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–Ω–∏–≥–µ —Å –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã.

    –§—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–Ω–∏–≥–∏ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:
    –Ω–∞–∑–≤–∞–Ω–∏–µ, —Ü–µ–Ω—É, —Ä–µ–π—Ç–∏–Ω–≥, –Ω–∞–ª–∏—á–∏–µ, –æ–ø–∏—Å–∞–Ω–∏–µ –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏.

    Args:
        url (str): URL-–∞–¥—Ä–µ—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∫–Ω–∏–≥–æ–π

    Returns:
        dict: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –æ –∫–Ω–∏–≥–µ, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Å–ª–µ–¥—É—é—â–∏–µ –∫–ª—é—á–∏:
            - 'title' (str): –ù–∞–∑–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏
            - 'price' (str): –¶–µ–Ω–∞ –∫–Ω–∏–≥–∏
            - 'rating' (str): –†–µ–π—Ç–∏–Ω–≥ –∫–Ω–∏–≥–∏ (One-Five)
            - 'availability' (str): –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –Ω–∞–ª–∏—á–∏–∏
            - 'description' (str): –û–ø–∏—Å–∞–Ω–∏–µ –∫–Ω–∏–≥–∏
            - 'product_info' (dict): –°–ª–æ–≤–∞—Ä—å —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã
    """
    response = requests.get(url)

    if response.status_code != 200:
        return {"error": f"Failed to fetch page. Status code: {response.status_code}"}

    soup = BeautifulSoup(response.text, "html.parser")
    book_data = {}

    title_element = soup.find("h1")
    book_data['title'] = title_element.text.strip() if title_element else "Not found"

    price_element = soup.find("p", class_="price_color")
    book_data['price'] = price_element.text.strip() if price_element else "Not found"

    rating_element = soup.find("p", class_="star-rating")
    if rating_element:
        book_data['rating'] = rating_element["class"][1]
    else:
        book_data['rating'] = "Not found"

    availability_element = soup.find("p", class_="instock")
    book_data['availability'] = availability_element.text.strip() if availability_element else "Not found"

    description_header = soup.find("div", id="product_description")
    if description_header:
        description_element = description_header.find_next_sibling("p")
        book_data['description'] = description_element.text.strip() if description_element else "Not found"
    else:
        book_data['description'] = "Not found"

    product_table = soup.find("table", class_="table-striped")
    book_data['product_info'] = {}

    if product_table:
        rows = product_table.find_all("tr")
        for row in rows:
            header = row.find("th")
            value = row.find("td")
            if header and value:
                book_data['product_info'][header.text.strip()] = value.text.strip()

    return book_data


def scrape_books(save_to_file=False, max_pages=None, delay=0.5):
    """
    –ü–∞—Ä—Å–∏—Ç –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–∞–ª–æ–≥–∞ books.toscrape.com
    –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–∞–Ω–Ω—ã—Ö –æ –∫–Ω–∏–≥–∞—Ö.

    Args:
        save_to_file (bool): –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–∞–π–ª
        max_pages (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        delay (float): –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
    """
    base_url = "http://books.toscrape.com/catalogue/page-{}.html"
    all_books = []
    page_number = 1

    print(" –ù–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü...")

    while True:
        if max_pages and page_number > max_pages:
            print(f" –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –≤ {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü")
            break

        url = base_url.format(page_number)
        response = requests.get(url)

        if response.status_code != 200:
            print(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω. –°—Ç—Ä–∞–Ω–∏—Ü –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {page_number - 1}")
            break

        print(f" –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_number}...")

        soup = BeautifulSoup(response.text, "html.parser")
        books = soup.select("h3 a")

        if not books:
            print(" –ë–æ–ª—å—à–µ –∫–Ω–∏–≥ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
            break

        books_count = 0
        for book in books:
            relative_link = book.get("href")
            if relative_link.startswith("../"):
                relative_link = relative_link.replace("../", "")
            book_url = f"http://books.toscrape.com/catalogue/{relative_link}"

            book_data = get_book_data(book_url)
            all_books.append(book_data)
            books_count += 1
            time.sleep(delay)

        print(f" –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_number} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {books_count} –∫–Ω–∏–≥")
        page_number += 1

    if save_to_file:
        artifacts_dir = "artifacts"
        os.makedirs(artifacts_dir, exist_ok=True)

        file_path = os.path.join(artifacts_dir, "books_data.txt")

        with open(file_path, "w", encoding="utf-8") as f:
            for book in all_books:
                f.write(str(book) + "\n\n")

        print(f" –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {file_path}")
        print(f" –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ –¥–∞–Ω–Ω—ã—Ö –æ {len(all_books)} –∫–Ω–∏–≥–∞—Ö —Å {page_number - 1} —Å—Ç—Ä–∞–Ω–∏—Ü")

    return all_books


def job():
    """
    –§—É–Ω–∫—Ü–∏—è, –∑–∞–ø—É—Å–∫–∞–µ—Ç –ø–∞—Ä—Å–∏–Ω–≥ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∫–∞–∂–¥—ã–π –¥–µ–Ω—å –≤ 19:00.
    """
    print("–ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞...")

    scrape_books(save_to_file=True, max_pages=3, delay=0.3)

    print("‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω –∏ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.")


schedule.every().day.at("20:05").do(job)

print("–ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∑–∞–ø—É—â–µ–Ω. –û–∂–∏–¥–∞–µ–º –∑–∞–ø—É—Å–∫ –∑–∞–¥–∞—á–∏...")
print("–î–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω–∞–∂–º–∏—Ç–µ Ctrl+C")

try:
    while True:
        schedule.run_pending()
        time.sleep(2)
except KeyboardInterrupt:
    print("\nüõë –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
